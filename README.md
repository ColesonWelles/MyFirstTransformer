# MyFirstTransformer
My implementation of an Encoder-Decoder Transformer model as outlined in the original "Attention Is All You Need" (Vaswani et al. 2017) paper. 

This encoder-decoder model is trained for language translation. I chose to keep things somewhat simple 
for this project and utilize a word-level tokenizer from HuggingFace instead of a more sophisticated BPE tokenizer. 